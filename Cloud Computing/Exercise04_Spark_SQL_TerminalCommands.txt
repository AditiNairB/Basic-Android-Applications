Student ID: 801104903	
Student Name: Balasundaram Avudai Nayagam



Using username "hadoop".
Authenticating with public key "imported-openssh-key"
Passphrase for key "imported-openssh-key":
Last login: Sun Feb 16 21:16:15 2020

       __|  __|_  )
       _|  (     /   Amazon Linux AMI
      ___|\___|___|

https://aws.amazon.com/amazon-linux-ami/2018.03-release-notes/
16 package(s) needed for security, out of 27 available
Run "sudo yum update" to apply all updates.

EEEEEEEEEEEEEEEEEEEE MMMMMMMM           MMMMMMMM RRRRRRRRRRRRRRR
E::::::::::::::::::E M:::::::M         M:::::::M R::::::::::::::R
EE:::::EEEEEEEEE:::E M::::::::M       M::::::::M R:::::RRRRRR:::::R
  E::::E       EEEEE M:::::::::M     M:::::::::M RR::::R      R::::R
  E::::E             M::::::M:::M   M:::M::::::M   R:::R      R::::R
  E:::::EEEEEEEEEE   M:::::M M:::M M:::M M:::::M   R:::RRRRRR:::::R
  E::::::::::::::E   M:::::M  M:::M:::M  M:::::M   R:::::::::::RR
  E:::::EEEEEEEEEE   M:::::M   M:::::M   M:::::M   R:::RRRRRR::::R
  E::::E             M:::::M    M:::M    M:::::M   R:::R      R::::R
  E::::E       EEEEE M:::::M     MMM     M:::::M   R:::R      R::::R
EE:::::EEEEEEEE::::E M:::::M             M:::::M   R:::R      R::::R
E::::::::::::::::::E M:::::M             M:::::M RR::::R      R::::R
EEEEEEEEEEEEEEEEEEEE MMMMMMM             MMMMMMM RRRRRRR      RRRRRR

[hadoop@ip-172-31-73-162 ~]$ aws s3 cp s3://scalabucket801104903/SparkSQLScala.jar .
download: s3://scalabucket801104903/SparkSQLScala.jar to ./SparkSQLScala.jar
[hadoop@ip-172-31-73-162 ~]$ spark-submit --class org.SparkSQL.Driver ./SparkSQLScala.jar s3://scalabucket801104903/data.txt s3://scalabucket801104903/SparkSQLOutput
20/02/16 21:38:18 INFO SparkContext: Running Spark version 2.4.4
20/02/16 21:38:18 INFO SparkContext: Submitted application: SparkAction
20/02/16 21:38:18 INFO SecurityManager: Changing view acls to: hadoop
20/02/16 21:38:18 INFO SecurityManager: Changing modify acls to: hadoop
20/02/16 21:38:18 INFO SecurityManager: Changing view acls groups to:
20/02/16 21:38:18 INFO SecurityManager: Changing modify acls groups to:
20/02/16 21:38:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hadoop); groups with view permissions: Set(); users  with modify permissions: Set(hadoop); groups with modify permissions: Set()
20/02/16 21:38:19 INFO Utils: Successfully started service 'sparkDriver' on port 35861.
20/02/16 21:38:19 INFO SparkEnv: Registering MapOutputTracker
20/02/16 21:38:19 INFO SparkEnv: Registering BlockManagerMaster
20/02/16 21:38:19 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/02/16 21:38:19 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/02/16 21:38:19 INFO DiskBlockManager: Created local directory at /mnt/tmp/blockmgr-49d26003-28de-44ed-b4ad-cedd936a990e
20/02/16 21:38:19 INFO MemoryStore: MemoryStore started with capacity 1038.8 MB
20/02/16 21:38:20 INFO SparkEnv: Registering OutputCommitCoordinator
20/02/16 21:38:20 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/02/16 21:38:21 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://ip-172-31-73-162.ec2.internal:4040
20/02/16 21:38:21 INFO SparkContext: Added JAR file:/home/hadoop/./SparkSQLScala.jar at spark://ip-172-31-73-162.ec2.internal:35861/jars/SparkSQLScala.jar with timestamp 1581889101131
20/02/16 21:38:21 INFO Executor: Starting executor ID driver on host localhost
20/02/16 21:38:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43135.
20/02/16 21:38:21 INFO NettyBlockTransferService: Server created on ip-172-31-73-162.ec2.internal:43135
20/02/16 21:38:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/02/16 21:38:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-172-31-73-162.ec2.internal, 43135, None)
20/02/16 21:38:21 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-73-162.ec2.internal:43135 with 1038.8 MB RAM, BlockManagerId(driver, ip-172-31-73-162.ec2.internal, 43135, None)
20/02/16 21:38:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-172-31-73-162.ec2.internal, 43135, None)
20/02/16 21:38:21 INFO BlockManager: external shuffle service port = 7337
20/02/16 21:38:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-172-31-73-162.ec2.internal, 43135, None)
20/02/16 21:38:24 INFO EventLoggingListener: Logging events to hdfs:/var/log/spark/apps/local-1581889101359
20/02/16 21:38:24 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 237.5 KB, free 1038.6 MB)
20/02/16 21:38:25 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 24.0 KB, free 1038.6 MB)
20/02/16 21:38:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-172-31-73-162.ec2.internal:43135 (size: 24.0 KB, free: 1038.8 MB)
20/02/16 21:38:25 INFO SparkContext: Created broadcast 0 from textFile at Driver.scala:19
20/02/16 21:38:29 INFO SharedState: loading hive config file: file:/etc/spark/conf.dist/hive-site.xml
20/02/16 21:38:29 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('hdfs:///user/spark/warehouse').
20/02/16 21:38:29 INFO SharedState: Warehouse path is 'hdfs:///user/spark/warehouse'.
20/02/16 21:38:30 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/02/16 21:38:33 INFO CodeGenerator: Code generated in 739.452558 ms
20/02/16 21:38:34 INFO ClientConfigurationFactory: Set initial getObject socket timeout to 2000 ms.
20/02/16 21:38:35 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
20/02/16 21:38:35 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.DirectFileOutputCommitter
20/02/16 21:38:35 INFO DirectFileOutputCommitter: Nothing to setup since the outputs are written directly.
20/02/16 21:38:36 INFO GPLNativeCodeLoader: Loaded native gpl library
20/02/16 21:38:36 INFO LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 5f788d5e8f90539ee331702c753fa250727128f4]
20/02/16 21:38:36 INFO FileInputFormat: Total input files to process : 1
20/02/16 21:38:36 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78
20/02/16 21:38:36 INFO DAGScheduler: Got job 0 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions
20/02/16 21:38:36 INFO DAGScheduler: Final stage: ResultStage 0 (runJob at SparkHadoopWriter.scala:78)
20/02/16 21:38:36 INFO DAGScheduler: Parents of final stage: List()
20/02/16 21:38:36 INFO DAGScheduler: Missing parents: List()
20/02/16 21:38:36 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[8] at saveAsTextFile at Driver.scala:31), which has no missing parents
20/02/16 21:38:36 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 95.2 KB, free 1038.5 MB)
20/02/16 21:38:36 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 34.5 KB, free 1038.4 MB)
20/02/16 21:38:36 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ip-172-31-73-162.ec2.internal:43135 (size: 34.5 KB, free: 1038.8 MB)
20/02/16 21:38:36 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1201
20/02/16 21:38:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[8] at saveAsTextFile at Driver.scala:31) (first 15 tasks are for partitions Vector(0))
20/02/16 21:38:36 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
20/02/16 21:38:36 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 7894 bytes)
20/02/16 21:38:36 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
20/02/16 21:38:36 INFO Executor: Fetching spark://ip-172-31-73-162.ec2.internal:35861/jars/SparkSQLScala.jar with timestamp 1581889101131
20/02/16 21:38:36 INFO TransportClientFactory: Successfully created connection to ip-172-31-73-162.ec2.internal/172.31.73.162:35861 after 76 ms (0 ms spent in bootstraps)
20/02/16 21:38:36 INFO Utils: Fetching spark://ip-172-31-73-162.ec2.internal:35861/jars/SparkSQLScala.jar to /mnt/tmp/spark-ca258330-cc2e-4577-acfe-1d1ca2ac7e32/userFiles-2e2380f2-5ed4-48aa-83c4-ae71e35c9035/fetchFileTemp1133283282474667042.tmp
20/02/16 21:38:36 INFO Executor: Adding file:/mnt/tmp/spark-ca258330-cc2e-4577-acfe-1d1ca2ac7e32/userFiles-2e2380f2-5ed4-48aa-83c4-ae71e35c9035/SparkSQLScala.jar to class loader
20/02/16 21:38:37 INFO HadoopRDD: Input split: s3://scalabucket801104903/data.txt:0+53593
20/02/16 21:38:37 INFO S3NativeFileSystem: Opening 's3://scalabucket801104903/data.txt' for reading
20/02/16 21:38:37 INFO CodeGenerator: Code generated in 44.955219 ms
20/02/16 21:38:37 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.DirectFileOutputCommitter
20/02/16 21:38:37 INFO MultipartUploadOutputStream: close closed:false s3://scalabucket801104903/SparkSQLOutput/part-00000
20/02/16 21:38:37 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200216213834_0008_m_000000_0
20/02/16 21:38:37 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1619 bytes result sent to driver
20/02/16 21:38:37 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1290 ms on localhost (executor driver) (1/1)
20/02/16 21:38:37 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
20/02/16 21:38:37 INFO DAGScheduler: ResultStage 0 (runJob at SparkHadoopWriter.scala:78) finished in 1.669 s
20/02/16 21:38:37 INFO DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:78, took 1.877230 s
20/02/16 21:38:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
20/02/16 21:38:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: true
20/02/16 21:38:38 INFO DirectFileOutputCommitter: Direct Write: ENABLED
20/02/16 21:38:38 INFO DirectFileOutputCommitter: Nothing to clean up since no temporary files were written.
20/02/16 21:38:38 INFO MultipartUploadOutputStream: close closed:false s3://scalabucket801104903/SparkSQLOutput/_SUCCESS
20/02/16 21:38:38 INFO SparkHadoopWriter: Job job_20200216213834_0008 committed.
20/02/16 21:38:38 INFO SparkContext: Invoking stop() from shutdown hook
20/02/16 21:38:38 INFO SparkUI: Stopped Spark web UI at http://ip-172-31-73-162.ec2.internal:4040
20/02/16 21:38:38 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/02/16 21:38:38 INFO MemoryStore: MemoryStore cleared
20/02/16 21:38:38 INFO BlockManager: BlockManager stopped
20/02/16 21:38:38 INFO BlockManagerMaster: BlockManagerMaster stopped
20/02/16 21:38:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/02/16 21:38:38 INFO SparkContext: Successfully stopped SparkContext
20/02/16 21:38:38 INFO ShutdownHookManager: Shutdown hook called
20/02/16 21:38:38 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-2034a6f1-c1c4-4dd6-9467-1db2612ef226
20/02/16 21:38:38 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-ca258330-cc2e-4577-acfe-1d1ca2ac7e32
[hadoop@ip-172-31-73-162 ~]$
